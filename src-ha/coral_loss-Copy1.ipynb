{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de53cd8a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1ed6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15715/2405149226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3\"\u001b[0m  \u001b[0;31m# either 3 or 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_generators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/lab_data2/kcochran/cs197/team_da/src-ha/data_generators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyfaidx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFasta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # either 3 or 6\n",
    "\n",
    "from data_generators import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3a88d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365\n",
      "1370\n"
     ]
    }
   ],
   "source": [
    "# setup generators / data loaders for training and validation\n",
    "\n",
    "# we'll make the training data loader in the training loop,\n",
    "# since we need to update some of the examples used each epoch\n",
    "source_train_gen = TrainGenerator(\"mouse\", \"CTCF\")\n",
    "target_train_gen = TrainGenerator(\"human\", \"CTCF\")\n",
    "\n",
    "source_val_gen = ValGenerator(\"mouse\", \"CTCF\")\n",
    "# using a batch size of 1 here because the generator returns\n",
    "# many examples in each batch\n",
    "source_val_data_loader = DataLoader(source_val_gen, batch_size = 1, shuffle = False)\n",
    "\n",
    "target_val_gen = ValGenerator(\"human\", \"CTCF\")\n",
    "target_val_data_loader = DataLoader(target_val_gen, batch_size = 1, shuffle = False) # why would shuffle=True mess with the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285fd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_diff = lambda A, B: [np.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "# cov = lambda data, convolve: np.cov(torch.max(convolve(data.squeeze().cuda()), 2).values.T.cpu().detach().numpy())\n",
    "\n",
    "norm_diff = lambda A, B: [torch.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "cov = lambda data, convolve: torch_cov(torch.max(convolve(data.squeeze().cuda()), 2).values.T)\n",
    "\n",
    "def loader_to_generator(data_loader):\n",
    "    for batch in data_loader:\n",
    "        yield batch\n",
    "\n",
    "def CORAL_loss(src_batch, tgt_gen, convolve):\n",
    "    # TODO need to handle case where we have more source than target data and next() returns None\n",
    "    tgt_batch, tgt_labels = next(tgt_gen)\n",
    "    a = cov(src_batch, convolve)\n",
    "    b = cov(tgt_batch, convolve)\n",
    "    d = a.shape[0]\n",
    "    loss = torch.tensor(norm_diff(a, b)).cuda() / (4 * d * d)\n",
    "    return torch.sum(loss) # TODO sum? mean?\n",
    "    # TODO check that each batch is the same as previous\n",
    "\n",
    "# def CORAL_loss_gen(src_gen, tgt_gen, conv_fn):\n",
    "#     src_batch, src_labels = next(src_gen)\n",
    "#     tgt_batch, tgt_labels = next(tgt_gen)\n",
    "#     if src_batch is None or tgt_batch is None:\n",
    "#         return -1\n",
    "#     a = conv_fn(src_batch.squeeze().cuda())\n",
    "#     b = conv_fn(tgt_batch.squeeze().cuda())\n",
    "#     return norm_diff(a, b)\n",
    "\n",
    "class BasicModel(torch.nn.Module):\n",
    "    def __init__(self, alpha1=1., alpha2=0.):\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.input_seq_len = 500\n",
    "        num_conv_filters = 240\n",
    "        lstm_hidden_units = 32\n",
    "        fc_layer1_units = 1024\n",
    "        fc_layer2_units = 512\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        \n",
    "        \n",
    "        # Defining the layers to go into our model\n",
    "        # (see the forward function for how they fit together)\n",
    "        self.conv = torch.nn.Conv1d(4, num_conv_filters, kernel_size=20, padding=0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpool = torch.nn.MaxPool1d(15, stride=15, padding=0)\n",
    "        self.lstm = torch.nn.LSTM(input_size=num_conv_filters,\n",
    "                                  hidden_size=lstm_hidden_units,\n",
    "                                  batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(in_features=lstm_hidden_units,\n",
    "                                   out_features=fc_layer1_units)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.fc2 = torch.nn.Linear(in_features=fc_layer1_units,\n",
    "                                   out_features=fc_layer2_units)\n",
    "        self.fc_final = torch.nn.Linear(in_features=fc_layer2_units,\n",
    "                                        out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.BCE_loss = torch.nn.BCELoss()\n",
    "\n",
    "        # We'll store performance metrics during training in these lists\n",
    "        self.train_loss_by_epoch = []\n",
    "        self.train_BCE_loss_by_epoch = []\n",
    "        self.train_CORAL_loss_by_epoch = []\n",
    "        self.source_val_loss_by_epoch = []\n",
    "        self.source_val_auprc_by_epoch = []\n",
    "        self.target_val_loss_by_epoch = []\n",
    "        self.target_val_auprc_by_epoch = []\n",
    "\n",
    "        # We'll record the best model we've seen yet each epoch\n",
    "        self.best_state_so_far = self.state_dict()\n",
    "        self.best_auprc_so_far = 1\n",
    "    \n",
    "        self.norm_diff = lambda A, B: [np.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "        self.cov = lambda data: np.cov(torch.max(self(data.squeeze().cuda()), 2).values.T.cpu().detach().numpy())\n",
    "        self.cov = lambda data: torch_cov(torch.max(self(data.squeeze().cuda()), 2).values.T.cpu().detach().numpy())\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # return (self.conv(X))\n",
    "        X_1 = self.relu(self.conv(X))\n",
    "        # LSTM is expecting input of shape (batches, seq_len, conv_filters)\n",
    "        X_2 = self.maxpool(X_1).permute(0, 2, 1)\n",
    "        X_3, _ = self.lstm(X_2)\n",
    "        X_4 = X_3[:, -1]  # only need final output of LSTM\n",
    "        X_5 = self.relu(self.fc1(X_4))\n",
    "        X_6 = self.dropout(X_5)\n",
    "        X_7 = self.sigmoid(self.fc2(X_6))\n",
    "        y = self.sigmoid(self.fc_final(X_7)).squeeze()\n",
    "        return y\n",
    "    \n",
    "    def validation(self, data_loader): \n",
    "        # only run this within torch.no_grad() context!\n",
    "        losses = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for seqs_onehot_batch, labels_batch in tqdm(data_loader):\n",
    "            # push batch through model, get predictions, calculate loss\n",
    "            preds_batch = self(seqs_onehot_batch.squeeze().cuda())\n",
    "            labels_batch = labels_batch.squeeze()\n",
    "            loss_batch = self.BCE_loss(preds_batch, labels_batch.cuda())\n",
    "            losses.append(loss_batch.item())\n",
    "\n",
    "            # storing labels + preds for auPRC calculation later\n",
    "            labels.extend(labels_batch.detach().numpy())  \n",
    "            preds.extend(preds_batch.cpu().detach().numpy())\n",
    "            \n",
    "        return np.array(losses), np.array(preds), np.array(labels)\n",
    "\n",
    "    def CORAL_loss_validation(self, source_val_data_loader, target_val_data_loader):\n",
    "        losses = []\n",
    "        tgt_gen = loader_to_generator(target_val_data_loader)\n",
    "        \n",
    "        for seqs_onehot_batch, labels_batch in tqdm(source_val_data_loader):\n",
    "            loss_batch = CORAL_loss(seqs_onehot_batch, tgt_gen, self.conv)\n",
    "            losses.append(loss_batch)\n",
    "        return torch.tensor(losses)\n",
    "#         src_gen = loader_to_generator(source_val_data_loader)\n",
    "#         tgt_gen = loader_to_generator(target_val_data_loader)\n",
    "#         src_batch,_ = next(src_gen)\n",
    "#         tgt_batch,_ = next(tgt_gen)\n",
    "#         while src_batch is not None and tgt_batch is not None:\n",
    "#             CORAL_loss_batch = self.alpha * CORAL_loss(src_batch.squeeze().cuda(), self.conv)\n",
    "#             CORAL_losses.append(CORAL_loss_batch.item())\n",
    "#             src_batch,_ = next(src_gen)\n",
    "#             tgt_batch,_ = next(tgt_gen)\n",
    "    \n",
    "    def fit(self, source_train_gen, target_train_gen, source_val_data_loader, target_val_data_loader,\n",
    "            optimizer, epochs=15):\n",
    "        # FOR DEBUGGING: SET HYPER-PARAMETERS HERE\n",
    "#         alpha1 = 0.\n",
    "#         alpha2 = 1.\n",
    "        print(f'Training for {epochs} epochs')\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            torch.cuda.empty_cache()  # clear memory to keep stuff from blocking up\n",
    "            \n",
    "            print(\"=== Epoch \" + str(epoch + 1) + \" ===\")\n",
    "            print(\"Training...\")\n",
    "            self.train()\n",
    "            \n",
    "            # using a batch size of 1 here because the generator returns\n",
    "            # many examples in each batch\n",
    "            source_train_data_loader = DataLoader(source_train_gen,\n",
    "                               batch_size = 1, shuffle = True)\n",
    "            target_train_data_loader = DataLoader(target_train_gen,\n",
    "                               batch_size = 1, shuffle = True)\n",
    "            \n",
    "            # returns the next batch of shuffled human data\n",
    "            target_train_data_generator = loader_to_generator(target_train_data_loader)\n",
    "\n",
    "            train_losses = []\n",
    "            train_BCE_losses = []\n",
    "            train_CORAL_losses = []\n",
    "            train_preds = []\n",
    "            train_labels = []\n",
    "            for seqs_onehot_batch, labels_batch in tqdm(source_train_data_loader):\n",
    "                # reset the optimizer; need to do each batch after weight update\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # push batch through model, get predictions, and calculate loss\n",
    "                preds = self(seqs_onehot_batch.squeeze().cuda())\n",
    "                labels_batch = labels_batch.squeeze()\n",
    "                BCE_loss_batch = self.BCE_loss(preds, labels_batch.cuda())\n",
    "                CORAL_loss_batch = CORAL_loss(seqs_onehot_batch, target_train_data_generator, self.conv)\n",
    "\n",
    "                # backpropagate the loss and update model weights accordingly\n",
    "                total_loss_batch = self.alpha1 * BCE_loss_batch + self.alpha2 * CORAL_loss_batch\n",
    "#                 print('total loss:', total_loss)\n",
    "#                 print(f'BCE: {BCE_loss_batch}, CORAL: {CORAL_loss_batch}')\n",
    "#                 CORAL_loss_batch.backward()\n",
    "                total_loss_batch.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_losses.append(total_loss_batch.item())\n",
    "                train_BCE_losses.append(BCE_loss_batch.item())\n",
    "                train_CORAL_losses.append(CORAL_loss_batch.item())\n",
    "                train_labels.extend(labels_batch)\n",
    "                train_preds.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            self.train_loss_by_epoch.append(np.mean(train_losses))\n",
    "            self.train_BCE_loss_by_epoch.append(np.mean(train_BCE_losses))\n",
    "            self.train_CORAL_loss_by_epoch.append(np.mean(train_CORAL_losses))\n",
    "            \n",
    "            print(f'avg total loss: {self.train_loss_by_epoch[-1]}')\n",
    "            print(f'avg BCE   loss: {self.train_BCE_loss_by_epoch[-1]}')\n",
    "            print(f'avg CORAL loss: {self.train_CORAL_loss_by_epoch[-1]}')\n",
    "            \n",
    "            print_metrics(train_preds, train_labels)\n",
    "            \n",
    "            # load new set of negative examples for next epoch\n",
    "            source_train_gen.on_epoch_end()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Since we don't use gradients during model evaluation,\n",
    "            # the following two lines let the model predict for many examples\n",
    "            # more efficiently (without having to keep track of gradients)\n",
    "            \n",
    "            continue\n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                # Assess model performance on same-species validation set\n",
    "                print(\"\\nEvaluating on source validation data...\")\n",
    "                \n",
    "                source_val_losses, source_val_preds, source_val_labels = self.validation(source_val_data_loader)\n",
    "\n",
    "                print(\"Source validation loss:\", np.mean(source_val_losses))\n",
    "                self.source_val_loss_by_epoch.append(np.mean(source_val_losses))\n",
    "\n",
    "                # calc auPRC over source validation set\n",
    "                source_val_auprc = print_metrics(source_val_preds, source_val_labels)\n",
    "                self.source_val_auprc_by_epoch.append(source_val_auprc)\n",
    "\n",
    "                # check if this is the best performance we've seen so far\n",
    "                # if yes, save the model weights -- we'll use the best model overall\n",
    "                # for later analyses\n",
    "                if source_val_auprc < self.best_auprc_so_far:\n",
    "                    self.best_auprc_so_far = source_val_auprc\n",
    "                    self.best_state_so_far = self.state_dict()\n",
    "                \n",
    "                \n",
    "                # now repeat for target species data \n",
    "                print(\"\\nEvaluating on target validation data...\")\n",
    "                \n",
    "                target_val_losses, target_val_preds, target_val_labels = self.validation(target_val_data_loader)\n",
    "\n",
    "                print(\"Target validation loss:\", np.mean(target_val_losses))\n",
    "                self.target_val_loss_by_epoch.append(np.mean(target_val_losses))\n",
    "\n",
    "                # calc auPRC over source validation set\n",
    "                target_val_auprc = print_metrics(target_val_preds, target_val_labels)\n",
    "                self.target_val_auprc_by_epoch.append(target_val_auprc)\n",
    "                \n",
    "            print(f'End of epoch {epoch + 1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "ALPHA1 = 0\n",
    "ALPHA2 = 1\n",
    "LR     = 0.000001\n",
    "print(f'  LR={LR}\\n  alpha1={ALPHA1} (BCE),\\n  alpha2={ALPHA2} (CORAL)')\n",
    "\n",
    "model = BasicModel(alpha1=ALPHA1, alpha2=ALPHA2) # 0, 100_000_000, np.inf\n",
    "# model = BasicModel() # setting alphas in the code for now\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# model.load_state_dict(torch.load('../models/baseline'))\n",
    "model.cuda()\n",
    "model.fit(source_train_gen, target_train_gen, source_val_data_loader, target_val_data_loader, optimizer, epochs = 15)\n",
    "model.cpu()\n",
    "timestamp = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "torch.save(model.state_dict(), f'../models/coral-alpha_{ALPHA2}_{timestamp}')\n",
    "# with open(f'../logs/coral-alpha_{ALPHA}_{timestamp}.txt', 'w') as file:\n",
    "#     file.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45676821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# TEST tqdm\n",
    "# import time\n",
    "# d = {0:1, 1:2, 2:3, 3:4, 4:5}\n",
    "# l = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "# def fn():\n",
    "#     for k,v in tqdm(l):\n",
    "#         time.sleep(0.1)\n",
    "\n",
    "# for i in tqdm(range(20)):\n",
    "#     fn()\n",
    "\n",
    "%%capture cap --no-stderr\n",
    "print('yay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2e82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs197-env] *",
   "language": "python",
   "name": "conda-env-cs197-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
