{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de53cd8a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1ed6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # either 3 or 6\n",
    "\n",
    "from data_generators import *\n",
    "from utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3a88d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365\n",
      "1370\n"
     ]
    }
   ],
   "source": [
    "# setup generators / data loaders for training and validation\n",
    "\n",
    "# we'll make the training data loader in the training loop,\n",
    "# since we need to update some of the examples used each epoch\n",
    "source_train_gen = TrainGenerator(\"mouse\", \"CTCF\")\n",
    "target_train_gen = TrainGenerator(\"human\", \"CTCF\")\n",
    "\n",
    "source_val_gen = ValGenerator(\"mouse\", \"CTCF\")\n",
    "# using a batch size of 1 here because the generator returns\n",
    "# many examples in each batch\n",
    "source_val_data_loader = DataLoader(source_val_gen, batch_size = 1, shuffle = False)\n",
    "\n",
    "target_val_gen = ValGenerator(\"human\", \"CTCF\")\n",
    "target_val_data_loader = DataLoader(target_val_gen, batch_size = 1, shuffle = False) # why would shuffle=True mess with the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285fd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_diff = lambda A, B: [np.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "# cov = lambda data, convolve: np.cov(torch.max(convolve(data.squeeze().cuda()), 2).values.T.cpu().detach().numpy())\n",
    "\n",
    "# norm_diff = lambda A, B: [torch.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "# cov = lambda data, convolve: torch_cov(torch.max(convolve(data.squeeze().cuda()), 2).values.T)\n",
    "\n",
    "# def loader_to_generator(data_loader):\n",
    "#     for batch in data_loader:\n",
    "#         yield batch\n",
    "\n",
    "# def CORAL_loss(src_batch, tgt_gen, convolve):\n",
    "#     # TODO need to handle case where we have more source than target data and next() returns None\n",
    "#     tgt_batch, tgt_labels = next(tgt_gen)\n",
    "#     a = cov(src_batch, convolve)\n",
    "#     b = cov(tgt_batch, convolve)\n",
    "#     d = a.shape[0]\n",
    "#     loss = torch.tensor(norm_diff(a, b)).cuda() / (4 * d * d)\n",
    "#     return torch.sum(loss) # TODO sum? mean?\n",
    "#     # TODO check that each batch is the same as previous\n",
    "\n",
    "# def CORAL_loss_gen(src_gen, tgt_gen, conv_fn):\n",
    "#     src_batch, src_labels = next(src_gen)\n",
    "#     tgt_batch, tgt_labels = next(tgt_gen)\n",
    "#     if src_batch is None or tgt_batch is None:\n",
    "#         return -1\n",
    "#     a = conv_fn(src_batch.squeeze().cuda())\n",
    "#     b = conv_fn(tgt_batch.squeeze().cuda())\n",
    "#     return norm_diff(a, b)\n",
    "\n",
    "class BasicModel(torch.nn.Module):\n",
    "    def __init__(self, alpha1=1., alpha2=0., summary_writer=None):\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.input_seq_len = 500\n",
    "        num_conv_filters = 240\n",
    "        lstm_hidden_units = 32\n",
    "        fc_layer1_units = 1024\n",
    "        fc_layer2_units = 512\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        \n",
    "        \n",
    "        # Defining the layers to go into our model\n",
    "        # (see the forward function for how they fit together)\n",
    "        self.conv = torch.nn.Conv1d(4, num_conv_filters, kernel_size=20, padding=0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpool = torch.nn.MaxPool1d(15, stride=15, padding=0)\n",
    "        self.lstm = torch.nn.LSTM(input_size=num_conv_filters,\n",
    "                                  hidden_size=lstm_hidden_units,\n",
    "                                  batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(in_features=lstm_hidden_units,\n",
    "                                   out_features=fc_layer1_units)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.fc2 = torch.nn.Linear(in_features=fc_layer1_units,\n",
    "                                   out_features=fc_layer2_units)\n",
    "        self.fc_final = torch.nn.Linear(in_features=fc_layer2_units,\n",
    "                                        out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.BCE_loss = torch.nn.BCELoss()\n",
    "\n",
    "        # We'll store performance metrics during training in these lists\n",
    "        self.train_loss_by_epoch = []\n",
    "        self.train_BCE_loss_by_epoch = []\n",
    "        self.train_CORAL_loss_by_epoch = []\n",
    "        self.source_val_loss_by_epoch = []\n",
    "        self.source_val_auprc_by_epoch = []\n",
    "        self.target_val_loss_by_epoch = []\n",
    "        self.target_val_auprc_by_epoch = []\n",
    "\n",
    "        # We'll record the best model we've seen yet each epoch\n",
    "        self.best_state_so_far = self.state_dict()\n",
    "        self.best_auprc_so_far = 1\n",
    "\n",
    "#         self.norm_diff = lambda A, B: [np.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "#         self.cov = lambda data: np.cov(torch.max(self(data.squeeze().cuda()), 2).values.T.cpu().detach().numpy())\n",
    "\n",
    "        self.norm_diff = lambda A, B: [torch.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "        self.cov = lambda data: torch_cov(torch.max(self(data.squeeze().cuda()), 2).values.T)\n",
    "    \n",
    "    def loader_to_generator(self, data_loader):\n",
    "        for batch in data_loader:\n",
    "            yield batch\n",
    "\n",
    "    def CORAL_loss(self, src_batch, tgt_gen, convolve):\n",
    "        # TODO need to handle case where we have more source than target data and next() returns None\n",
    "        tgt_batch, tgt_labels = next(tgt_gen)\n",
    "        a = cov(src_batch, self.conv)\n",
    "        b = cov(tgt_batch, self.conv)\n",
    "        d = a.shape[0]\n",
    "        loss = torch.tensor(self.norm_diff(a, b)).cuda() / (4 * d * d)\n",
    "        return torch.sum(loss) # TODO sum? mean?\n",
    "        # TODO check that each batch is the same as previous\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # return (self.conv(X))\n",
    "        X_1 = self.relu(self.conv(X))\n",
    "        # LSTM is expecting input of shape (batches, seq_len, conv_filters)\n",
    "        X_2 = self.maxpool(X_1).permute(0, 2, 1)\n",
    "        X_3, _ = self.lstm(X_2)\n",
    "        X_4 = X_3[:, -1]  # only need final output of LSTM\n",
    "        X_5 = self.relu(self.fc1(X_4))\n",
    "        X_6 = self.dropout(X_5)\n",
    "        X_7 = self.sigmoid(self.fc2(X_6))\n",
    "        y = self.sigmoid(self.fc_final(X_7)).squeeze()\n",
    "        return y\n",
    "    \n",
    "    def validation(self, data_loader): \n",
    "        # only run this within torch.no_grad() context!\n",
    "        losses = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for seqs_onehot_batch, labels_batch in tqdm(data_loader):\n",
    "            # push batch through model, get predictions, calculate loss\n",
    "            preds_batch = self(seqs_onehot_batch.squeeze().cuda())\n",
    "            labels_batch = labels_batch.squeeze()\n",
    "            loss_batch = self.BCE_loss(preds_batch, labels_batch.cuda())\n",
    "            losses.append(loss_batch.item())\n",
    "\n",
    "            # storing labels + preds for auPRC calculation later\n",
    "            labels.extend(labels_batch.detach().numpy())  \n",
    "            preds.extend(preds_batch.cpu().detach().numpy())\n",
    "            \n",
    "        return np.array(losses), np.array(preds), np.array(labels)\n",
    "\n",
    "    def CORAL_loss_validation(self, source_val_data_loader, target_val_data_loader):\n",
    "        losses = []\n",
    "        tgt_gen = loader_to_generator(target_val_data_loader)\n",
    "        \n",
    "        for seqs_onehot_batch, labels_batch in tqdm(source_val_data_loader):\n",
    "            loss_batch = CORAL_loss(seqs_onehot_batch, tgt_gen, self.conv)\n",
    "            losses.append(loss_batch)\n",
    "        return torch.tensor(losses)\n",
    "    \n",
    "#         src_gen = loader_to_generator(source_val_data_loader)\n",
    "#         tgt_gen = loader_to_generator(target_val_data_loader)\n",
    "#         src_batch,_ = next(src_gen)\n",
    "#         tgt_batch,_ = next(tgt_gen)\n",
    "#         while src_batch is not None and tgt_batch is not None:\n",
    "#             CORAL_loss_batch = self.alpha * CORAL_loss(src_batch.squeeze().cuda(), self.conv)\n",
    "#             CORAL_losses.append(CORAL_loss_batch.item())\n",
    "#             src_batch,_ = next(src_gen)\n",
    "#             tgt_batch,_ = next(tgt_gen)\n",
    "    \n",
    "    def fit(self, source_train_gen, target_train_gen, source_val_data_loader, target_val_data_loader,\n",
    "            optimizer, epochs=15):\n",
    "        print(f'Training for {epochs} epochs')\n",
    "        CORAL_loss_all = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            torch.cuda.empty_cache()  # clear memory to keep stuff from blocking up\n",
    "            \n",
    "            print(\"=== Epoch \" + str(epoch + 1) + \" ===\")\n",
    "            print(\"Training...\")\n",
    "            self.train()\n",
    "            \n",
    "            # using a batch size of 1 here because the generator returns\n",
    "            # many examples in each batch\n",
    "            source_train_data_loader = DataLoader(source_train_gen,\n",
    "                               batch_size = 1, shuffle = True)\n",
    "            target_train_data_loader = DataLoader(target_train_gen,\n",
    "                               batch_size = 1, shuffle = True)\n",
    "            \n",
    "            # returns the next batch of shuffled human data\n",
    "            target_train_data_generator = loader_to_generator(target_train_data_loader)\n",
    "\n",
    "            train_losses = []\n",
    "            train_BCE_losses = []\n",
    "            train_CORAL_losses = []\n",
    "            train_preds = []\n",
    "            train_labels = []\n",
    "            for batch_i, data in enumerate(tqdm(source_train_data_loader)):\n",
    "#                 for p in model.parameters():\n",
    "#                     print(p)\n",
    "                seqs_onehot_batch, labels_batch = data\n",
    "                \n",
    "                # reset the optimizer; need to do each batch after weight update\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # push batch through model, get predictions, and calculate loss\n",
    "                preds = self(seqs_onehot_batch.squeeze().cuda())\n",
    "                labels_batch = labels_batch.squeeze()\n",
    "                BCE_loss_batch = self.BCE_loss(preds, labels_batch.cuda())\n",
    "                CORAL_loss_batch = self.CORAL_loss(seqs_onehot_batch, target_train_data_generator, self.conv)\n",
    "                CORAL_loss_batch.requires_grad=True\n",
    "                # backpropagate the loss and update model weights accordingly\n",
    "                total_loss_batch = BCE_loss_batch # CORAL_loss_batch\n",
    "#                 total_loss_batch = self.alpha1 * BCE_loss_batch + self.alpha2 * CORAL_loss_batch\n",
    "#                 print('total loss:', total_loss)\n",
    "#                 print(f'BCE: {BCE_loss_batch}, CORAL: {CORAL_loss_batch}')\n",
    "#                 CORAL_loss_batch.backward()\n",
    "                total_loss_batch.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_losses.append(total_loss_batch.item())\n",
    "                train_BCE_losses.append(BCE_loss_batch.item())\n",
    "                train_CORAL_losses.append(CORAL_loss_batch.item())\n",
    "                train_labels.extend(labels_batch)\n",
    "                train_preds.extend(preds.cpu().detach().numpy())\n",
    "                summary_writer.add_scalar(\"Loss/train/CORAL\", CORAL_loss_batch.item(), batch_i + epoch * 400) # 400 is batch size for train TODO get rid of magic number\n",
    "                summary_writer.add_scalar(\"Loss/train/BCE\", BCE_loss_batch.item(), batch_i + epoch * 400) # 400 is batch size for train TODO get rid of magic number\n",
    "\n",
    "#                 print('train CORAL loss:', train_CORAL_losses[-1])\n",
    "            \n",
    "            CORAL_loss_all.extend(train_CORAL_losses)\n",
    "            self.train_loss_by_epoch.append(np.mean(train_losses))\n",
    "            self.train_BCE_loss_by_epoch.append(np.mean(train_BCE_losses))\n",
    "            self.train_CORAL_loss_by_epoch.append(np.mean(train_CORAL_losses))\n",
    "            \n",
    "            print(f'avg total loss: {self.train_loss_by_epoch[-1]}')\n",
    "            print(f'avg BCE   loss: {self.train_BCE_loss_by_epoch[-1]}')\n",
    "            print(f'avg CORAL loss: {self.train_CORAL_loss_by_epoch[-1]}')\n",
    "            \n",
    "            print_metrics(train_preds, train_labels)\n",
    "            \n",
    "            # load new set of negative examples for next epoch\n",
    "            source_train_gen.on_epoch_end()\n",
    "            \n",
    "            # TODO plot train CORAL loss by batches ?        \n",
    "            summary_writer.flush()\n",
    "            \n",
    "            # Since we don't use gradients during model evaluation,\n",
    "            # the following two lines let the model predict for many examples\n",
    "            # more efficiently (without having to keep track of gradients)\n",
    "#             return CORAL_loss_all\n",
    "            \n",
    "            continue\n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                # Assess model performance on same-species validation set\n",
    "                print(\"\\nEvaluating on source validation data...\")\n",
    "                \n",
    "                source_val_losses, source_val_preds, source_val_labels = self.validation(source_val_data_loader)\n",
    "\n",
    "                print(\"Source validation loss:\", np.mean(source_val_losses))\n",
    "                self.source_val_loss_by_epoch.append(np.mean(source_val_losses))\n",
    "\n",
    "                # calc auPRC over source validation set\n",
    "                source_val_auprc = print_metrics(source_val_preds, source_val_labels)\n",
    "                self.source_val_auprc_by_epoch.append(source_val_auprc)\n",
    "\n",
    "                # check if this is the best performance we've seen so far\n",
    "                # if yes, save the model weights -- we'll use the best model overall\n",
    "                # for later analyses\n",
    "                if source_val_auprc < self.best_auprc_so_far:\n",
    "                    self.best_auprc_so_far = source_val_auprc\n",
    "                    self.best_state_so_far = self.state_dict()\n",
    "                \n",
    "                \n",
    "                # now repeat for target species data \n",
    "                print(\"\\nEvaluating on target validation data...\")\n",
    "                \n",
    "                target_val_losses, target_val_preds, target_val_labels = self.validation(target_val_data_loader)\n",
    "\n",
    "                print(\"Target validation loss:\", np.mean(target_val_losses))\n",
    "                self.target_val_loss_by_epoch.append(np.mean(target_val_losses))\n",
    "\n",
    "                # calc auPRC over source validation set\n",
    "                target_val_auprc = print_metrics(target_val_preds, target_val_labels)\n",
    "                self.target_val_auprc_by_epoch.append(target_val_auprc)\n",
    "                \n",
    "            print(f'End of epoch {epoch + 1}')\n",
    "        \n",
    "        return CORAL_loss_all # after all epochs end\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LR=0.01\n",
      "  alpha1=1 (BCE),\n",
      "  alpha2=0.076923077 (CORAL)\n",
      "Training for 1 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764d40f0fcca461b91560abedccc8cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd45f8c027f4c15b9f95082bf77d786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "ALPHA1 = 1\n",
    "ALPHA2 = 0.076923077\n",
    "LR     = 1e-2\n",
    "print(f'  LR={LR}\\n  alpha1={ALPHA1} (BCE),\\n  alpha2={ALPHA2} (CORAL)')\n",
    "summary_writer = SummaryWriter()\n",
    "model = BasicModel(alpha1=ALPHA1, alpha2=ALPHA2, summary_writer=summary_writer) # 0, 100_000_000, np.inf\n",
    "model.load_state_dict(torch.load('../models/baseline'))\n",
    "# model = BasicModel() # setting alphas in the code for now\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# model.load_state_dict(torch.load('../models/baseline'))\n",
    "model.cuda()\n",
    "CORAL_loss_all = model.fit(source_train_gen, target_train_gen, source_val_data_loader, target_val_data_loader, optimizer, epochs = 1)\n",
    "model.cpu()\n",
    "summary_writer.close()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%tT%H%M%S')\n",
    "torch.save(model.state_dict(), f'../models/coral-{timestamp}_alpha_{ALPHA2}')\n",
    "# with open(f'../logs/coral-alpha_{ALPHA}_{timestamp}.txt', 'w') as file:\n",
    "#     file.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45676821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST tqdm\n",
    "# import time\n",
    "# d = {0:1, 1:2, 2:3, 3:4, 4:5}\n",
    "# l = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "# def fn():\n",
    "#     for k,v in tqdm(l):\n",
    "#         time.sleep(0.1)\n",
    "\n",
    "# for i in tqdm(range(20)):\n",
    "#     fn()\n",
    "\n",
    "# %%capture cap --no-stderr\n",
    "# print('yay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9a2e82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.884203420360008e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(CORAL_loss_all)\n",
    "\n",
    "y = CORAL_loss_all\n",
    "x = range(len(CORAL_loss_all))\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, m*x + b)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "187dd22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-7.5079e-04,  4.6684e-04, -3.5735e-04,  ..., -1.4589e-05,\n",
      "          -1.5906e-04, -4.1398e-05],\n",
      "         [ 7.3603e-04, -2.4121e-03, -1.7109e-03,  ...,  3.8792e-04,\n",
      "          -1.7809e-03, -2.6763e-03],\n",
      "         [-2.3683e-03, -7.5917e-04, -9.8219e-04,  ..., -2.0722e-03,\n",
      "          -1.7481e-03,  1.8491e-05],\n",
      "         [-1.2286e-03, -9.0721e-04, -5.6117e-04,  ..., -1.9127e-03,\n",
      "           7.6470e-05, -9.1238e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.9547e-04,  5.0688e-04,  1.5468e-03,  ...,  8.4451e-04,\n",
      "          -1.3620e-04,  6.1275e-04],\n",
      "         [ 1.8832e-03,  3.9011e-04, -9.9140e-04,  ..., -1.0352e-05,\n",
      "           2.2558e-03,  4.8715e-04],\n",
      "         [ 3.6240e-04,  2.9111e-04,  9.2816e-04,  ..., -7.6898e-05,\n",
      "           3.9971e-04,  8.2042e-04],\n",
      "         [-5.9499e-04,  1.4580e-03,  1.1625e-03,  ...,  1.8888e-03,\n",
      "           1.2672e-04,  7.2575e-04]],\n",
      "\n",
      "        [[ 1.1913e-03,  2.4019e-03, -8.2345e-04,  ...,  2.9400e-03,\n",
      "           1.2718e-03, -1.0736e-04],\n",
      "         [ 4.7934e-03,  1.6185e-03,  4.2928e-03,  ...,  1.8687e-05,\n",
      "           2.2073e-03, -6.5519e-04],\n",
      "         [ 2.6244e-05, -3.0410e-05,  1.5502e-04,  ...,  3.7175e-03,\n",
      "           1.8156e-03,  5.5646e-03],\n",
      "         [ 1.1554e-03,  3.1763e-03,  3.5419e-03,  ...,  4.9014e-04,\n",
      "           1.8717e-03,  2.3643e-03]],\n",
      "\n",
      "        [[ 2.9300e-03,  1.0547e-04,  1.9004e-03,  ...,  1.3496e-03,\n",
      "           3.9543e-04,  1.1518e-03],\n",
      "         [ 3.9349e-03,  2.8923e-03,  4.2602e-03,  ...,  1.5473e-03,\n",
      "           1.9268e-03, -6.2594e-04],\n",
      "         [-5.3987e-04,  1.5219e-03,  1.4145e-03,  ...,  4.7583e-03,\n",
      "           5.9391e-03,  5.9413e-03],\n",
      "         [ 3.4239e-03,  5.2292e-03,  2.1739e-03,  ...,  2.0938e-03,\n",
      "           1.4876e-03,  3.2818e-03]]])\n"
     ]
    }
   ],
   "source": [
    "print(model.conv.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs197-env] *",
   "language": "python",
   "name": "conda-env-cs197-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
