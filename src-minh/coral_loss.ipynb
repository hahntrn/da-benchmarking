{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de53cd8a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1ed6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # either 3 or 6\n",
    "\n",
    "from data_generators import *\n",
    "from utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed3a88d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365\n",
      "1370\n"
     ]
    }
   ],
   "source": [
    "# setup generators / data loaders for training and validation\n",
    "\n",
    "# we'll make the training data loader in the training loop,\n",
    "# since we need to update some of the examples used each epoch\n",
    "source_train_gen = TrainGenerator(\"mouse\", \"CTCF\")\n",
    "target_train_gen = TrainGenerator(\"human\", \"CTCF\")\n",
    "\n",
    "source_val_gen = ValGenerator(\"mouse\", \"CTCF\")\n",
    "# using a batch size of 1 here because the generator returns\n",
    "# many examples in each batch\n",
    "source_val_data_loader = DataLoader(source_val_gen, batch_size = 1, shuffle = False)\n",
    "\n",
    "target_val_gen = ValGenerator(\"human\", \"CTCF\")\n",
    "target_val_data_loader = DataLoader(target_val_gen, batch_size = 1, shuffle = False) # why would shuffle=True mess with the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "285fd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_diff = lambda A, B: [np.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "cov = lambda data, convolve: np.cov(torch.max(convolve(data.squeeze().cuda()), 2).values.T.cpu().detach().numpy())\n",
    "\n",
    "def loader_to_generator(data_loader):\n",
    "    for batch in data_loader:\n",
    "        yield batch\n",
    "\n",
    "def CORAL_loss(src_batch, tgt_gen, convolve):\n",
    "    # TODO need to handle case where we have more source than target data and next() returns None\n",
    "    tgt_batch, tgt_labels = next(tgt_gen)\n",
    "    a = cov(src_batch, convolve)\n",
    "    b = cov(tgt_batch, convolve)\n",
    "    loss = torch.tensor(norm_diff(a, b)).cuda()\n",
    "    return torch.sum(loss) # TODO sum? mean?\n",
    "\n",
    "def CORAL_loss_gen(src_gen, tgt_gen, conv_fn):\n",
    "    src_batch, src_labels = next(src_gen)\n",
    "    tgt_batch, tgt_labels = next(tgt_gen)\n",
    "    if src_batch is None or tgt_batch is None:\n",
    "        return -1\n",
    "    a = conv_fn(src_batch.squeeze().cuda())\n",
    "    b = conv_fn(tgt_batch.squeeze().cuda())\n",
    "    return norm_diff(a, b)\n",
    "\n",
    "class BasicModel(torch.nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.input_seq_len = 500\n",
    "        num_conv_filters = 240\n",
    "        lstm_hidden_units = 32\n",
    "        fc_layer1_units = 1024\n",
    "        fc_layer2_units = 512\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "        # Defining the layers to go into our model\n",
    "        # (see the forward function for how they fit together)\n",
    "        self.conv = torch.nn.Conv1d(4, num_conv_filters, kernel_size=20, padding=0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpool = torch.nn.MaxPool1d(15, stride=15, padding=0)\n",
    "        self.lstm = torch.nn.LSTM(input_size=num_conv_filters,\n",
    "                                  hidden_size=lstm_hidden_units,\n",
    "                                  batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(in_features=lstm_hidden_units,\n",
    "                                   out_features=fc_layer1_units)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.fc2 = torch.nn.Linear(in_features=fc_layer1_units,\n",
    "                                   out_features=fc_layer2_units)\n",
    "        self.fc_final = torch.nn.Linear(in_features=fc_layer2_units,\n",
    "                                        out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.loss = torch.nn.BCELoss()\n",
    "\n",
    "        # We'll store performance metrics during training in these lists\n",
    "        self.train_loss_by_epoch = []\n",
    "        self.train_CORAL_loss_by_epoch = []\n",
    "        self.source_val_loss_by_epoch = []\n",
    "        self.source_val_CORAL_loss_by_epoch = []\n",
    "        self.source_val_auprc_by_epoch = []\n",
    "        self.target_val_loss_by_epoch = []\n",
    "        self.target_val_auprc_by_epoch = []\n",
    "\n",
    "        # We'll record the best model we've seen yet each epoch\n",
    "        self.best_state_so_far = self.state_dict()\n",
    "        self.best_auprc_so_far = 1\n",
    "    \n",
    "        self.norm_diff = lambda A, B: [np.linalg.norm(a - b) for (a,b) in zip(A,B)]\n",
    "        self.cov = lambda data: np.cov(torch.max(self(data.squeeze().cuda()), 2).values.T.cpu().detach().numpy())\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # return (self.conv(X))\n",
    "        X_1 = self.relu(self.conv(X))\n",
    "        # LSTM is expecting input of shape (batches, seq_len, conv_filters)\n",
    "        X_2 = self.maxpool(X_1).permute(0, 2, 1)\n",
    "        X_3, _ = self.lstm(X_2)\n",
    "        X_4 = X_3[:, -1]  # only need final output of LSTM\n",
    "        X_5 = self.relu(self.fc1(X_4))\n",
    "        X_6 = self.dropout(X_5)\n",
    "        X_7 = self.sigmoid(self.fc2(X_6))\n",
    "        y = self.sigmoid(self.fc_final(X_7)).squeeze()\n",
    "        return y\n",
    "    \n",
    "    def validation(self, data_loader): \n",
    "        # only run this within torch.no_grad() context!\n",
    "        losses = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for seqs_onehot_batch, labels_batch in tqdm(data_loader):\n",
    "            # push batch through model, get predictions, calculate loss\n",
    "            preds_batch = self(seqs_onehot_batch.squeeze().cuda())\n",
    "            labels_batch = labels_batch.squeeze()\n",
    "            loss_batch = self.loss(preds_batch, labels_batch.cuda())\n",
    "            losses.append(loss_batch.item())\n",
    "\n",
    "            # storing labels + preds for auPRC calculation later\n",
    "            labels.extend(labels_batch.detach().numpy())  \n",
    "            preds.extend(preds_batch.cpu().detach().numpy())\n",
    "            \n",
    "        return np.array(losses), np.array(preds), np.array(labels)\n",
    "\n",
    "    def CORAL_loss_validation(self, source_val_data_loader, target_val_data_loader):\n",
    "        losses = []\n",
    "        tgt_gen = loader_to_generator(target_val_data_loader)\n",
    "        \n",
    "        for seqs_onehot_batch, labels_batch in tqdm(source_val_data_loader):\n",
    "            loss_batch = CORAL_loss(seqs_onehot_batch, tgt_gen, self.conv)\n",
    "            losses.append(loss_batch)\n",
    "        return torch.tensor(losses)\n",
    "#         src_gen = loader_to_generator(source_val_data_loader)\n",
    "#         tgt_gen = loader_to_generator(target_val_data_loader)\n",
    "#         src_batch,_ = next(src_gen)\n",
    "#         tgt_batch,_ = next(tgt_gen)\n",
    "#         while src_batch is not None and tgt_batch is not None:\n",
    "#             CORAL_loss_batch = self.alpha * CORAL_loss(src_batch.squeeze().cuda(), self.conv)\n",
    "#             CORAL_losses.append(CORAL_loss_batch.item())\n",
    "#             src_batch,_ = next(src_gen)\n",
    "#             tgt_batch,_ = next(tgt_gen)\n",
    "    \n",
    "    def fit(self, source_train_gen, target_train_gen, source_val_data_loader, target_val_data_loader,\n",
    "            optimizer, epochs=15):\n",
    "        print(epochs)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            torch.cuda.empty_cache()  # clear memory to keep stuff from blocking up\n",
    "            \n",
    "            print(\"=== Epoch \" + str(epoch + 1) + \" ===\")\n",
    "            print(\"Training...\")\n",
    "            self.train()\n",
    "            \n",
    "            # using a batch size of 1 here because the generator returns\n",
    "            # many examples in each batch\n",
    "            source_train_data_loader = DataLoader(source_train_gen,\n",
    "                               batch_size = 1, shuffle = True)\n",
    "            target_train_data_loader = DataLoader(target_train_gen,\n",
    "                               batch_size = 1, shuffle = True)\n",
    "            \n",
    "            # returns the next batch of shuffled human data\n",
    "            target_train_data_generator = loader_to_generator(target_train_data_loader)\n",
    "\n",
    "            train_losses = []\n",
    "            train_CORAL_losses = []\n",
    "            train_preds = []\n",
    "            train_labels = []\n",
    "            for seqs_onehot_batch, labels_batch in tqdm(source_train_data_loader):\n",
    "                # reset the optimizer; need to do each batch after weight update\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # push batch through model, get predictions, and calculate loss\n",
    "                preds = self(seqs_onehot_batch.squeeze().cuda())\n",
    "                labels_batch = labels_batch.squeeze()\n",
    "                loss_batch = self.loss(preds, labels_batch.cuda())\n",
    "                \n",
    "                CORAL_loss_batch = self.alpha * CORAL_loss(seqs_onehot_batch, target_train_data_generator, self.conv)\n",
    "                # brackpropogate the loss and update model weights accordingly\n",
    "                total_loss = loss_batch + CORAL_loss_batch\n",
    "                total_loss.backward()\n",
    "#                 loss_batch.backward()\n",
    "#                 CORAL_loss_batch.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_losses.append(loss_batch.item())\n",
    "                train_CORAL_losses.append(CORAL_loss_batch.item())\n",
    "                train_labels.extend(labels_batch)\n",
    "                train_preds.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            self.train_loss_by_epoch.append(np.mean(train_losses))\n",
    "            self.train_CORAL_loss_by_epoch.append(np.mean(train_CORAL_losses))\n",
    "            print_metrics(train_preds, train_labels)\n",
    "            \n",
    "            # load new set of negative examples for next epoch\n",
    "            source_train_gen.on_epoch_end()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Since we don't use gradients during model evaluation,\n",
    "            # the following two lines let the model predict for many examples\n",
    "            # more efficiently (without having to keep track of gradients)\n",
    "            \n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                # Assess model performance on same-species validation set\n",
    "                print(\"\\nEvaluating on source validation data...\")\n",
    "                \n",
    "                source_val_losses, source_val_preds, source_val_labels = self.validation(source_val_data_loader)\n",
    "\n",
    "                print(\"Validation loss:\", np.mean(source_val_losses))\n",
    "                self.source_val_loss_by_epoch.append(np.mean(source_val_losses))\n",
    "\n",
    "                # calc auPRC over source validation set\n",
    "                source_val_auprc = print_metrics(source_val_preds, source_val_labels)\n",
    "                self.source_val_auprc_by_epoch.append(source_val_auprc)\n",
    "\n",
    "                # check if this is the best performance we've seen so far\n",
    "                # if yes, save the model weights -- we'll use the best model overall\n",
    "                # for later analyses\n",
    "                if source_val_auprc < self.best_auprc_so_far:\n",
    "                    self.best_auprc_so_far = source_val_auprc\n",
    "                    self.best_state_so_far = self.state_dict()\n",
    "                \n",
    "                \n",
    "                # now repeat for target species data \n",
    "                print(\"\\nEvaluating on target validation data...\")\n",
    "                \n",
    "                target_val_losses, target_val_preds, target_val_labels = self.validation(target_val_data_loader)\n",
    "\n",
    "                print(\"Validation loss:\", np.mean(target_val_losses))\n",
    "                self.target_val_loss_by_epoch.append(np.mean(target_val_losses))\n",
    "\n",
    "                # calc auPRC over source validation set\n",
    "                target_val_auprc = print_metrics(target_val_preds, target_val_labels)\n",
    "                self.target_val_auprc_by_epoch.append(target_val_auprc)\n",
    "                \n",
    "            print(f'End of epoch {epoch}')\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d2fb4e8a404ca9bdcdc4c4fc9746c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cdd7a3c61b42c6a1d15a6f7793634b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\t 0.28535469238126476\n",
      "auROC:\t 0.9502864551047243\n",
      "auPRC:\t 0.9508320700426777\n",
      "Confusion Matrix (at t = 0.5):\n",
      " [[240514  32486]\n",
      " [ 34521 238479]]\n",
      "\n",
      "Evaluating on source validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac1110c9e984fa4aa92ec9096976d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "ALPHA = 0\n",
    "model = BasicModel(alpha=ALPHA) # 0, 100_000_000, np.inf\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# model.load_state_dict(torch.load('../models/baseline'))\n",
    "model.cuda()\n",
    "model.fit(source_train_gen, target_train_gen, source_val_data_loader, target_val_data_loader, optimizer, epochs = 1)\n",
    "model.cpu()\n",
    "timestamp = datetime.now().strftime('%Y%M%dT%H%M%S')\n",
    "torch.save(model.state_dict(), f'../models/coral-alpha_{ALPHA}_{timestamp}')\n",
    "# with open(f'../logs/coral-alpha_{ALPHA}_{timestamp}.txt', 'w') as file:\n",
    "#     file.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45676821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# TEST tqdm\n",
    "# import time\n",
    "# d = {0:1, 1:2, 2:3, 3:4, 4:5}\n",
    "# l = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "# def fn():\n",
    "#     for k,v in tqdm(l):\n",
    "#         time.sleep(0.1)\n",
    "\n",
    "# for i in tqdm(range(20)):\n",
    "#     fn()\n",
    "\n",
    "%%capture cap --no-stderr\n",
    "print('yay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2e82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs197-env] *",
   "language": "python",
   "name": "conda-env-cs197-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
